{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10692a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent huge warning messages of bertmodel \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "tqdm.pandas()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('scripts/')\n",
    "\n",
    "with open('API_KEY.txt') as f:\n",
    "    API_KEY = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89d3ec",
   "metadata": {},
   "source": [
    "About [preprocessing]( https://github.com/MaartenGr/BERTopic/issues/40), in words of Maarten Grootendorst, author of BERTopic:\n",
    "\n",
    "\n",
    "_\"In general, no, you do not need to preprocess your data. Like you said, keeping the original structure of the text is especially important for transformer-based models to understand the context._\n",
    "\n",
    "_However, there are exceptions to this. For example, if you were to have scraped documents with a lot of html tags, then it might be beneficial to remove those as they do not provide any interesting context.\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee428b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 878/878 [00:00<00:00, 1003.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of news: 878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from preprocess import filter_by_media\n",
    "from preprocess import cluster_by_month\n",
    "from preprocess import find_cities\n",
    "\n",
    "df = pd.read_csv(\"data/loslagos-comunas.csv\")[:1000]\n",
    "df = cluster_by_month(filter_by_media(df))\n",
    "df = df.drop_duplicates(subset='content', keep=\"first\")\n",
    "df.drop(columns=['comuna'], axis=1, inplace=True)\n",
    "df['cities'] =  df.content.progress_apply(lambda x: find_cities(str(x)))\n",
    "docs = df.content.tolist()\n",
    "\n",
    "print(\"number of news:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce13cbc",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "### \"Document Clustering\" with [BERTopic](https://github.com/MaartenGr/BERTopic) (+[SentenceTransformer](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) +[Word Embeddings](https://github.com/dccuchile/spanish-word-embeddings)) & Topic Representation with [Chat-gpt gpt-3.5-turbo\t](https://platform.openai.com/docs/models/gpt-3) + [Maximal Marginal Relevance](https://medium.com/tech-that-works/maximal-marginal-relevance-to-rerank-results-in-unsupervised-keyphrase-extraction-22d95015c7c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd196dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from bertopic.backend import WordDocEmbedder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "ft  = KeyedVectors.load_word2vec_format(\"data/SBW-vectors-300-min5.bin.gz\", binary=True) \n",
    "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "word_doc_embedder = WordDocEmbedder(embedding_model=embedding_model, word_embedding_model=ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9cebbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import OpenAI, MaximalMarginalRelevance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words=None)\n",
    "    \n",
    "umap_model = UMAP(n_neighbors=15, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=42)\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=10, \n",
    "                        metric='euclidean', \n",
    "                        cluster_selection_method='eom', \n",
    "                        prediction_data=True, \n",
    "                        min_samples=5)\n",
    "\n",
    "openai.api_key = API_KEY[0]\n",
    "\n",
    "# Create your representation model\n",
    "prompt = \"\"\"I have a topic that contains the following documents: \n",
    "[DOCUMENTS]\n",
    "The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "Based on the information above, extract in spanish a short topic label in the following format:\n",
    "topic: <topic label>\n",
    "\"\"\"\n",
    "\n",
    "openai_generator = OpenAI(model=\"gpt-3.5-turbo\", prompt=prompt, delay_in_seconds=15, chat=True) #chatgtp\n",
    "#openai_generator = OpenAI(prompt=prompt, delay_in_seconds=10)\n",
    "\n",
    "mmr = MaximalMarginalRelevance(diversity=0.3)\n",
    "representation_models = [mmr, openai_generator]\n",
    "\n",
    "topic_model = BERTopic(n_gram_range=(1,3),\n",
    "                       top_n_words=15, \n",
    "                      # nr_topics=50, #prevent RateLimitErrors\n",
    "                       embedding_model=word_doc_embedder,\n",
    "                       language=\"multilingual\", \n",
    "                       vectorizer_model=vectorizer_model,\n",
    "                       umap_model=umap_model,\n",
    "                       hdbscan_model=hdbscan_model,\n",
    "                       calculate_probabilities=True,\n",
    "                       verbose=True,\n",
    "                       representation_model=representation_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b15ab7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calculate_probabilities': True,\n",
       " 'ctfidf_model': ClassTfidfTransformer(),\n",
       " 'embedding_model': <bertopic.backend._word_doc.WordDocEmbedder at 0x1aa9dec90c8>,\n",
       " 'hdbscan_model': HDBSCAN(min_cluster_size=10, min_samples=5, prediction_data=True),\n",
       " 'language': None,\n",
       " 'low_memory': False,\n",
       " 'min_topic_size': 10,\n",
       " 'n_gram_range': (1, 3),\n",
       " 'nr_topics': None,\n",
       " 'representation_model': [MaximalMarginalRelevance(diversity=0.3),\n",
       "  OpenAI(chat=True, delay_in_seconds=15, model='gpt-3.5-turbo',\n",
       "         prompt='I have a topic that contains the following documents: \\n'\n",
       "                '[DOCUMENTS]\\n'\n",
       "                'The topic is described by the following keywords: [KEYWORDS]\\n'\n",
       "                '\\n'\n",
       "                'Based on the information above, extract in spanish a short '\n",
       "                'topic label in the following format:\\n'\n",
       "                'topic: <topic label>\\n')],\n",
       " 'seed_topic_list': None,\n",
       " 'top_n_words': 15,\n",
       " 'umap_model': UMAP(metric='cosine', min_dist=0.0, n_components=5, random_state=42),\n",
       " 'vectorizer_model': CountVectorizer(ngram_range=(1, 3)),\n",
       " 'verbose': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "422af852",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95dc0123d5cb4837ad52751af0d38cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 17:35:47,075 - BERTopic - Transformed documents to Embeddings\n",
      "2023-03-07 17:35:49,524 - BERTopic - Reduced dimensionality\n",
      "2023-03-07 17:35:49,580 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID b1e47ec718812e62653e4b32126d5453 in your email.) {\n  \"error\": {\n    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID b1e47ec718812e62653e4b32126d5453 in your email.)\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID b1e47ec718812e62653e4b32126d5453 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 07 Mar 2023 20:36:10 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'Access-Control-Allow-Origin': '*', 'Openai-Model': 'gpt-3.5-turbo-0301', 'Openai-Organization': 'user-s2u1conyucfknmjysvf1mezm', 'Openai-Processing-Ms': '844', 'Openai-Version': '2020-10-01', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains', 'X-Request-Id': 'b1e47ec718812e62653e4b32126d5453'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2568\\774354964.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_topic_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\56981\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\bertopic\\_bertopic.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, documents, embeddings, y)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;31m# Extract topics by calculating c-TF-IDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extract_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;31m# Reduce topics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\56981\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\bertopic\\_bertopic.py\u001b[0m in \u001b[0;36m_extract_topics\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m   2979\u001b[0m         \u001b[0mdocuments_per_topic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Topic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Document'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2980\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_tf_idf_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_tf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments_per_topic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2981\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopic_representations_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extract_words_per_topic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2982\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_topic_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2983\u001b[0m         self.topic_labels_ = {key: f\"{key}_\" + \"_\".join([word[0] for word in values[:4]])\n",
      "\u001b[1;32mc:\\users\\56981\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\bertopic\\_bertopic.py\u001b[0m in \u001b[0;36m_extract_words_per_topic\u001b[1;34m(self, words, documents, c_tf_idf)\u001b[0m\n\u001b[0;32m   3199\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepresentation_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3200\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtuner\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepresentation_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3201\u001b[1;33m                 \u001b[0mtopics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_tf_idf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3202\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepresentation_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseRepresentation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3203\u001b[0m             \u001b[0mtopics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepresentation_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_tf_idf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\56981\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\bertopic\\representation\\_openai.py\u001b[0m in \u001b[0;36mextract_topics\u001b[1;34m(self, topic_model, documents, c_tf_idf, topics)\u001b[0m\n\u001b[0;32m    167\u001b[0m                 ]\n\u001b[0;32m    168\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"messages\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator_kwargs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                 \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChatCompletion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"choices\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"message\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"content\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"topic: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\56981\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\56981\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mrequest_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             \u001b[0mrequest_timeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         )\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\56981\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         )\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\56981\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    621\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m                     \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m                 ),\n\u001b[0;32m    625\u001b[0m                 \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\56981\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             raise self.handle_error_response(\n\u001b[1;32m--> 680\u001b[1;33m                 \u001b[0mrbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m             )\n\u001b[0;32m    682\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAPIError\u001b[0m: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID b1e47ec718812e62653e4b32126d5453 in your email.) {\n  \"error\": {\n    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID b1e47ec718812e62653e4b32126d5453 in your email.)\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID b1e47ec718812e62653e4b32126d5453 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Tue, 07 Mar 2023 20:36:10 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'Access-Control-Allow-Origin': '*', 'Openai-Model': 'gpt-3.5-turbo-0301', 'Openai-Organization': 'user-s2u1conyucfknmjysvf1mezm', 'Openai-Processing-Ms': '844', 'Openai-Version': '2020-10-01', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains', 'X-Request-Id': 'b1e47ec718812e62653e4b32126d5453'}"
     ]
    }
   ],
   "source": [
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "clusters = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854385a1",
   "metadata": {},
   "source": [
    "We generate a dataframe with the obtained clusters and extract their most significant tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97442524",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters['most_freq_tokens'] = clusters.Topic.progress_apply(lambda x: topic_model.get_topic(x))\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f2152",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17509b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusters.to_csv('data/clusters.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95565ee",
   "metadata": {},
   "source": [
    "We label the news with their clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic_name'] = \"\"\n",
    "df['topic_number'] = \"\"\n",
    "\n",
    "# label each row with his topic\n",
    "labels=[]\n",
    "for item in topic_model.generate_topic_labels():\n",
    "    item.partition(\"_\")[2]\n",
    "    labels.append(item)\n",
    "\n",
    "count = 0\n",
    "for doc in tqdm(docs):  \n",
    "    df.at[df.index[df['content'] == doc], 'topic_name'] = labels[topics[count]+1]\n",
    "    df.at[df.index[df['content'] == doc], 'topic_number'] = topics[count]\n",
    "    count+=1\n",
    "    \n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dd0a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('data/labeled_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a4ec78",
   "metadata": {},
   "source": [
    "#### Evaluation: Coherence Score\n",
    "\n",
    "There is no one way to determine whether the coherence score is good or bad. The score and its value depends on the data that it's calculated from. For instance, in one case, the score of 0.5 might be good enough but in another case not acceptable. The only rule is that we want to **maximize** the score.\n",
    "\n",
    "Usually, the coherence score will increase with the number of topics . This increase will become smaller as the number of topics get higher. The trade-off between the number of topics and coherence score can be achieved using the so-called elbow technique. The method implies plotting coherence score as a function of number of topics. We use the elbow of the curve to select the number of topics.\n",
    "\n",
    "The idea behind this method is that we want to choose a point after which the diminishing increase of coherence score is no longer worth the additional increase of number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1973aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coherence_eval import umass_npmi\n",
    "\n",
    "umass_coherence, c_npmi_coherence = umass_npmi(docs, topics, topic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b190deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "umass_coherence, c_npmi_coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d79b20",
   "metadata": {},
   "source": [
    "#### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1685aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy as sch\n",
    "\n",
    "# Hierarchical topics\n",
    "linkage_function = lambda x: sch.linkage(x, 'ward', optimal_ordering=True)\n",
    "hierarchical_topics = topic_model.hierarchical_topics(docs, linkage_function=linkage_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964291f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 20, 'display.max_colwidth', 50)\n",
    "hierarchical_topics.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e339c50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb112e",
   "metadata": {},
   "source": [
    "#### Topics over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b846585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = df.date.tolist()\n",
    "topics_over_time = topic_model.topics_over_time(docs=docs, \n",
    "                                                timestamps=timestamps, \n",
    "                                                global_tuning=False, \n",
    "                                                evolution_tuning=False, \n",
    "                                                nr_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837789f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b88e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0419e1",
   "metadata": {},
   "source": [
    "### 5 _most important_ keywords of documents using [KeyBERT](https://github.com/MaartenGr/KeyBERT) (+[Word Embeddings](https://github.com/dccuchile/spanish-word-embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9d81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyword_extraction import extract_ngram_keywords\n",
    "\n",
    "df['2gram_keywords'] = extract_ngram_keywords((2,2), ft, docs)\n",
    "df['3gram_keywords'] = extract_ngram_keywords((3,3), ft, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c01521",
   "metadata": {},
   "source": [
    "### Sentiment Analysis using [BETO](https://huggingface.co/finiteautomata/beto-sentiment-analysis?text=Te+quiero.+Te+amo.) + Sentiment Analysis/Emotion Analysis using [roBERTuito](https://huggingface.co/pysentimiento/robertuito-sentiment-analysis?text=Te+quiero.+Te+amo.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BETO\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "model_name = \"finiteautomata/beto-sentiment-analysis\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce81b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# roBERTuito\n",
    "from pysentimiento import create_analyzer\n",
    "sentiment_analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce1e0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sentiment_analysis import sentiment_analysis\n",
    "df = sentiment_analysis(df, sentiment_analyzer, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2bbb52",
   "metadata": {},
   "source": [
    "### Manual classification of clusters according to areas of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f57f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def tokens_to_list(text):\n",
    "    text = text[1:-1]\n",
    "    res = ast.literal_eval(text)\n",
    "    return list(dict(res).keys())\n",
    "\n",
    "\n",
    "clusters = clusters[1:]\n",
    "clusters= clusters.set_index('Name')\n",
    "\n",
    "clusters['tokens'] = clusters.most_freq_tokens.apply(lambda x: str(tokens_to_list(\"{\"+str(x)[1:-1]+\"}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f26b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "health = clusters[clusters['tokens'].str.contains('salud|cáncer')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_news = pd.DataFrame()\n",
    "for index, rows in health.iterrows():\n",
    "    health_news = pd.concat([health_news, df[df.topic_number == rows['Topic']]])\n",
    "health_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e475c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "tex\n",
    "\n",
    "list(sentiment_analyzer.predict(row['title']).probas.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0501439a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
