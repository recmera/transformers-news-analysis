{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69ae83a",
   "metadata": {},
   "source": [
    "todo:\n",
    "1. probar coherencia con distintos hiperparametros (o simplemente decidirse por uno -> me gusta esta opción)\n",
    "2. generar los ministerios (clusterizacion de topicos - > usando hierarchical topics es más simple, sin embargo, algunos topícos (de niveles más altos) presentan ruido y fusionan temas totalmente distintos (e.g. futbol + salud???))\n",
    "3. generar dataset noticias y dataset topics, empezar a contextualizar los datos en el tiempo y por comuna. \n",
    "4. estudiar sobre knowledge graph -> me tinca mucho :p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10692a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent huge warning messages of bertmodel \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "tqdm.pandas()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('scripts/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89d3ec",
   "metadata": {},
   "source": [
    "About [preprocessing]( https://github.com/MaartenGr/BERTopic/issues/40), in words of Maarten Grootendorst, author of BERTopic:\n",
    "\n",
    "\n",
    "_\"In general, no, you do not need to preprocess your data. Like you said, keeping the original structure of the text is especially important for transformer-based models to understand the context._\n",
    "\n",
    "_However, there are exceptions to this. For example, if you were to have scraped documents with a lot of html tags, then it might be beneficial to remove those as they do not provide any interesting context.\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee428b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from preprocess import filter_by_media\n",
    "from preprocess import cluster_by_month\n",
    "from preprocess import find_cities\n",
    "\n",
    "df = pd.read_csv(\"data/loslagos-comunas.csv\")[:100]\n",
    "df = cluster_by_month(filter_by_media(df))\n",
    "df = df.drop_duplicates(subset='content', keep=\"first\")\n",
    "df.drop(columns=['comuna'], axis=1, inplace=True)\n",
    "df['cities'] =  df.content.progress_apply(lambda x: find_cities(str(x)))\n",
    "docs = df.content.tolist()\n",
    "\n",
    "print(\"number of news:\", len(df))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce13cbc",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "### Topic modeling with [BERTopic](https://github.com/MaartenGr/BERTopic) (+[SentenceTransformer](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) +[Word Embeddings](https://github.com/dccuchile/spanish-word-embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd196dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from bertopic.backend import WordDocEmbedder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "ft  = KeyedVectors.load_word2vec_format(\"data/SBW-vectors-300-min5.bin.gz\", binary=True) \n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "word_doc_embedder = WordDocEmbedder(embedding_model=embedding_model, word_embedding_model=ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b15ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from topic_modeling import model_definition\n",
    "\n",
    "topic_model = model_definition(word_doc_embedder)\n",
    "topic_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422af852",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "clusters = topic_model.get_topic_info()\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a4ec78",
   "metadata": {},
   "source": [
    "#### Evaluation: Coherence Score\n",
    "\n",
    "There is no one way to determine whether the coherence score is good or bad. The score and its value depends on the data that it's calculated from. For instance, in one case, the score of 0.5 might be good enough but in another case not acceptable. The only rule is that we want to **maximize** the score.\n",
    "\n",
    "Usually, the coherence score will increase with the number of topics . This increase will become smaller as the number of topics get higher. The trade-off between the number of topics and coherence score can be achieved using the so-called elbow technique. The method implies plotting coherence score as a function of number of topics. We use the elbow of the curve to select the number of topics.\n",
    "\n",
    "The idea behind this method is that we want to choose a point after which the diminishing increase of coherence score is no longer worth the additional increase of number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1973aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Preprocess Documents\n",
    "documents = pd.DataFrame({\"Document\": docs,\n",
    "                          \"ID\": range(len(docs)),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "# Evaluate\n",
    "cv_coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "\n",
    "umass_coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "c_npmi_coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "\n",
    "cv_coherence = cv_coherence_model.get_coherence()\n",
    "umass_coherence = umass_coherence_model.get_coherence()\n",
    "c_npmi_coherence = c_npmi_coherence_model.get_coherence()\n",
    "\n",
    "print(cv_coherence, umass_coherence, c_npmi_coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f1ad2",
   "metadata": {},
   "source": [
    "#### Topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters['most_freq_tokens'] = clusters.Topic.progress_apply(lambda x: topic_model.get_topic(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3271d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e648db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic'] = \"\"\n",
    "\n",
    "# label each row with his topic\n",
    "labels=[]\n",
    "for item in topic_model.generate_topic_labels():\n",
    "    item.partition(\"_\")[2]\n",
    "    labels.append(item)\n",
    "\n",
    "count = 0\n",
    "for doc in tqdm(docs):  \n",
    "    df.at[df.index[df['content'] == doc], 'topic'] = labels[topics[count]+1]\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37397ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d79b20",
   "metadata": {},
   "source": [
    "#### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1685aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy as sch\n",
    "\n",
    "# Hierarchical topics\n",
    "linkage_function = lambda x: sch.linkage(x, 'ward', optimal_ordering=True)\n",
    "hierarchical_topics = topic_model.hierarchical_topics(docs, linkage_function=linkage_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964291f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 20, 'display.max_colwidth', 50)\n",
    "hierarchical_topics.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e339c50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig=topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "fig.write_image(\"img/htopics2.png\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d638df0",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/rickiwasho/proyecto-titulo/main/img/htopics2xddddd.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9ae20",
   "metadata": {},
   "source": [
    "health=hierarchical_topics[hierarchical_topics['Parent_Name'] == \"casos_contagios_salud_dosis_casos activos\"].Topics\n",
    "health2=hierarchical_topics[hierarchical_topics['Parent_Name'] == \"cáncer_pacientes_salud_enfermedad_enfermedades\"].Topics\n",
    "sports=hierarchical_topics[hierarchical_topics['Parent_Name'] == \"partido_equipo_club_torneo_final\"].Topics\n",
    "russia_ucraine=hierarchical_topics[hierarchical_topics['Parent_Name'] == \"ucrania_rusia_ruso_putin_guerra\"].Topics\n",
    "seafood=hierarchical_topics[hierarchical_topics['Parent_Name'] == \"mariscos_marea roja_pesca_toxinas_extracción\"].Topics\n",
    "crime=hierarchical_topics[hierarchical_topics['Parent_Name'] == \"tribunal_fiscal_juicio_fiscalía_víctima\"].Topics\n",
    "world=hierarchical_topics[hierarchical_topics['Parent_Name'] == \"johnson_bolivia_gobierno_primer ministro_peso argentino\"].Topics\n",
    "politics=hierarchical_topics[hierarchical_topics['Parent_Name'] == \"presidente_gobierno_constitucional_comisión_votos\"].Topics\n",
    "art=hierarchical_topics[hierarchical_topics['Parent_Name'] == \"música_artista_artistas_arte_festival\"].Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_model.save(\"out/save2\", save_embedding_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f1f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_model = BERTopic.load(\"out/save2\")\n",
    "#new_topics, new_probs = my_model.transform(docs)\n",
    "#my_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb112e",
   "metadata": {},
   "source": [
    "#### Topics over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b846585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = df.date.tolist()\n",
    "\n",
    "# Es muy costoso (creo que se debe al word embedding)\n",
    "topics_over_time = topic_model.topics_over_time(docs=docs, \n",
    "                                                timestamps=timestamps, \n",
    "                                                global_tuning=False, \n",
    "                                                evolution_tuning=False, \n",
    "                                                nr_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837789f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b88e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = topic_model.visualize_topics_over_time(topics_over_time, topics=art)\n",
    "fig.write_image(\"img/dtmt2v.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f72320",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/rickiwasho/proyecto-titulo/main/img/dtmt2v.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0419e1",
   "metadata": {},
   "source": [
    "### 5 _most important_ keywords of documents using [KeyBERT](https://github.com/MaartenGr/KeyBERT) (+[SentenceTransformer](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) +[Word Embeddings](https://github.com/dccuchile/spanish-word-embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9d81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyword_extraction import extract_ngram_keywords\n",
    "\n",
    "df['2gram_keywords'] = extract_ngram_keywords((2,2), word_doc_embedder, docs)\n",
    "df['3gram_keywords'] = extract_ngram_keywords((3,3), word_doc_embedder, docs)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c01521",
   "metadata": {},
   "source": [
    "### Sentiment Analysis using [BETO](https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased?text=Mi+nombre+es+%5BMASK%5D+y+vivo+en+Nueva+York.) + Sentiment Analysis/Emotional Analysis using [roBERTuito](https://huggingface.co/pysentimiento/robertuito-sentiment-analysis?text=Te+quiero.+Te+amo.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e5658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pysentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd1a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roBERTuito\n",
    "from pysentimiento import create_analyzer\n",
    "sentiment_analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "emotion_analyzer = create_analyzer(task=\"emotion\", lang=\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BETO\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "model_name = \"finiteautomata/beto-sentiment-analysis\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_sentiment_roBERTuito'] = \"\"\n",
    "df['title_emotion_roBERTuito'] = \"\"\n",
    "df['title_sentiment_BETO'] = \"\"\n",
    "df['text_sentiment_BETO'] = \"\"\n",
    "\n",
    "for index, row in tqdm(sub.iterrows(), desc='sub rows - sentiment', total=sub.shape[0]):\n",
    "    # análisis del título de la noticia\n",
    "    sub.at[index, \"title_sentiment_roBERTuito\"] = sentiment_analyzer.predict(row['title'])\n",
    "    sub.at[index, \"title_emotion_roBERTuito\"] = emotion_analyzer.predict(row['title'])\n",
    "    sub.at[index, 'title_sentiment_BETO'] = nlp(row['title'])\n",
    "    \n",
    "    # análisis del cuerpo de la noticia\n",
    "    count_neutral = 0\n",
    "    count_negative = 0\n",
    "    count_positive = 0\n",
    "    partition = row['text'].split(\".\")\n",
    "    for text in partition:\n",
    "        # Analizamos su sentimiento\n",
    "        sentiment_value = nlp(text)\n",
    "        if sentiment_value[0].get('label') == \"NEU\": count_neutral=count_neutral+1\n",
    "        if sentiment_value[0].get('label') == \"NEG\": count_negative=count_negative+1\n",
    "        if sentiment_value[0].get('label') == \"POS\": count_positive=count_positive+1\n",
    "            \n",
    "    sub.at[index, \"text_sentiment_BETO\"] = {\"NEU\": count_neutral, \"NEG\": count_negative, \"POS\": count_positive}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 100, 'display.max_colwidth', None)\n",
    "sub[['title','title_sentiment_roBERTuito', 'title_emotion_roBERTuito','title_sentiment_BETO',\"text_sentiment_BETO\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
