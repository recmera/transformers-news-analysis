{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b3b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21afd6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "\n",
    "def extract_spanish_complex_keywords(nlp, text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    # COUNT FREQUENCY - COMPLEX WORD\n",
    "\n",
    "    pattern_1 = [{\"POS\": \"NOUN\"}, {\"LOWER\": \"de\"}, {\"POS\": \"NOUN\"}]\n",
    "    matcher.add(\"NOUN-de-NOUN\", [pattern_1])\n",
    "    pattern_2 = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADJ\"}]\n",
    "    matcher.add(\"NOUN-ADJ\", [pattern_2])\n",
    "    pattern_3 = [{\"POS\": \"NOUN\"}, {\"LOWER\": \"del\"}, {\"POS\": \"NOUN\"}]\n",
    "    matcher.add(\"NOUN-del-NOUN\", [pattern_3])\n",
    "    pattern_4 = [{\"POS\": \"NOUN\"}, {\"LOWER\": \"de\"},\n",
    "                 {\"LOWER\": \"la\"}, {\"POS\": \"NOUN\"}]\n",
    "    matcher.add(\"NOUN-de-la-NOUN\", [pattern_4])\n",
    "\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    complex_words = []\n",
    "    for match_id, start, end in matches:\n",
    "        #string_id = nlp.vocab.strings[match_id]\n",
    "        span = doc[start:end]  # The matched span\n",
    "        complex_word = span.text\n",
    "        complex_words.append(complex_word)\n",
    "\n",
    "    return complex_words\n",
    "\n",
    "\n",
    "def concat_text_cols(row, text_columns):\n",
    "    content = ''\n",
    "    for col in text_columns:\n",
    "        if isinstance(row[col], str):\n",
    "            content += row[col] + '. '\n",
    "    return content\n",
    "\n",
    "\n",
    "def extract_tokens_from_text(text, nlp, dframcy, lowercase=True):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    complex_keywords = extract_spanish_complex_keywords(nlp, text)\n",
    "    docs = dframcy.nlp(text)\n",
    "    anotation_dataframe, entity_dataframe = dframcy.to_dataframe(docs,\n",
    "                                                                 separate_entity_dframe=True)\n",
    "    verbs = anotation_dataframe.loc[anotation_dataframe['token_tag_']\n",
    "                                    == \"VERB\"]['token_text']\n",
    "    nouns = anotation_dataframe.loc[anotation_dataframe['token_tag_']\n",
    "                                    == \"NOUN\"]['token_text']\n",
    "    return (complex_keywords, verbs.tolist(), nouns.tolist())\n",
    "\n",
    "\n",
    "def concat_text_cols(row, text_columns):\n",
    "    content = ''\n",
    "    for col in text_columns:\n",
    "        if isinstance(row[col], str):\n",
    "            content += row[col] + '. '\n",
    "    return content\n",
    "\n",
    "\n",
    "def extract_df_tokens_inplace(df,\n",
    "                              nlp,\n",
    "                              dframcy,\n",
    "                              text_columns=['subject',\n",
    "                                            'mail_text', 'pdf_text'],\n",
    "                              lowercase=True):\n",
    "    df[[\"complex_keywords\", \"nouns\", \"verbs\"]] = \"\"\n",
    "    df['content'] = df.apply(\n",
    "        lambda row: concat_text_cols(row, text_columns), axis=1)\n",
    "    if lowercase:\n",
    "        df['content'] = df['content'].str.lower()\n",
    "    print(\"EXTRACTING TOKENS\")\n",
    "    for index, row in tqdm(df.iterrows(), desc='df rows - Keywords', total=df.shape[0]):\n",
    "        complex_keywords = extract_spanish_complex_keywords(nlp,\n",
    "                                                            row['content'])\n",
    "        df.at[index, \"complex_keywords\"] = complex_keywords\n",
    "        docs = dframcy.nlp(row['content'])\n",
    "        anotation_dataframe, entity_dataframe = dframcy.to_dataframe(docs,\n",
    "                                                                     separate_entity_dframe=True)\n",
    "        verbs = anotation_dataframe.loc[anotation_dataframe['token_tag_']\n",
    "                                        == \"VERB\"]['token_text']\n",
    "        nouns = anotation_dataframe.loc[anotation_dataframe['token_tag_']\n",
    "                                        == \"NOUN\"]['token_text']\n",
    "        df.at[index, \"verbs\"] = verbs.tolist()\n",
    "        df.at[index, \"nouns\"] = nouns.tolist()\n",
    "    df.drop(columns=['content'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3df395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dframcy import DframCy\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "dframcy = DframCy(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fadc614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(row, nlp, dframcy, text_columns=['subject', 'mail_text', 'pdf_text']):\n",
    "    content = ''\n",
    "    for col in text_columns:\n",
    "        if isinstance(row[col], str):\n",
    "            content += row[col] + '. '\n",
    "    complex_kws, verbs, nouns = extract_tokens_from_text(content, nlp, dframcy)\n",
    "    row['complex_keywords'] = complex_kws\n",
    "    row['verbs'] = verbs\n",
    "    row['nouns'] = nouns\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564b8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b0c7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d33b65c",
   "metadata": {},
   "source": [
    "https://github.com/TeamSophia2/FinTree/blob/a474a948e6d4c050358e71ae963b3acd821a8553/notebooks/demos/normalize.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
